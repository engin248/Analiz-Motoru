import asyncio
import pandas as pd
from playwright.async_api import async_playwright
from rich.console import Console
from datetime import datetime
import random
import os

# Kendi modÃ¼llerimiz
# Kendi modÃ¼llerimiz
import sys
import os

# Paths setup to reach LangChain_backend
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
backend_path = os.path.join(parent_dir, 'LangChain_backend')
sys.path.append(backend_path)

from src.scrapers.search_scraper import SearchScraper
from src.utils.stealth import apply_stealth

# DB Imports
from app.core.database import SessionLocal
from app.models.product import Product
from app.models.daily_metric import DailyMetric

console = Console()

async def harvest_links(keyword: str, max_pages: int = 200):
    """
    Belirtilen kelime iÃ§in sayfalarÄ± gezer ve linkleri toplar.
    """
    date_str = datetime.now().strftime("%Y-%m-%d")
    output_file = f"linkler_{keyword}_{date_str}.xlsx"
    
    # Cookie/State yÃ¶netimi (Burada da hafÄ±zayÄ± kullanalÄ±m)
    USER_DATA_DIR = "user_data"
    STATE_FILE = os.path.join(USER_DATA_DIR, "state.json")
    
    all_links = []
    seen_links = set()
    
    console.print(f"[bold green]ğŸŒ± Link HasadÄ± BaÅŸlÄ±yor: '{keyword}' (Hedef: {max_pages} Sayfa)[/bold green]")

    async with async_playwright() as p:
        # TarayÄ±cÄ±yÄ± baÅŸlat (HEADLESS MODE - 2x HÄ±z ArtÄ±ÅŸÄ±)
        browser = await p.chromium.launch(
            headless=True,  # âœ… Optimizasyon: False->True (GÃ¶rsel mod kapalÄ±)
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # Context ayarlarÄ±
        context_args = {
            'viewport': {'width': 1920, 'height': 1080},
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
        }
        
        # Cookie yÃ¼kle (Varsa)
        if os.path.exists(STATE_FILE):
            context_args['storage_state'] = STATE_FILE
        
        context = await browser.new_context(**context_args)
        
        # ğŸ•µï¸â€â™‚ï¸ Gizlilik Modu
        await apply_stealth(context)
        
        page = await context.new_page()
        scraper = SearchScraper(page)
        
        try:
            for page_num in range(1, max_pages + 1):
                console.print(f"[cyan]ğŸ“„ Sayfa Taraniyor: {page_num}/{max_pages}[/cyan]")
                
                links = await scraper.get_product_links(keyword, page_num)
                
                # Redirect kontrolÃ¼ (URL BazlÄ±)
                if links is None:
                    console.print(f"[bold yellow]ğŸ”„ URL YÃ¶nlendirmesi algÄ±landÄ±. Oturum sÄ±fÄ±rlanÄ±p tekrar deneniyor...[/bold yellow]")
                    # --- OTURUM YENÄ°LEME ---
                    await context.close()
                    await asyncio.sleep(2)
                    context = await browser.new_context(**context_args)
                    await apply_stealth(context)
                    page = await context.new_page()
                    scraper = SearchScraper(page)
                    console.print(f"[cyan]ğŸ”„ Yeni oturumla {page_num}. sayfa tekrar deneniyor...[/cyan]")
                    links = await scraper.get_product_links(keyword, page_num)
                    if links is None:
                        console.print(f"[bold red]â›” Yeni oturumda da yÃ¶nlendirme devam ediyor. Pes edildi.[/bold red]")
                        break

                if not links:
                    console.print(f"[red]âš ï¸  Sayfa {page_num} boÅŸ veya hata alÄ±ndÄ±.[/red]")
                    if page_num > 5: break
                    continue

                # --- Ä°Ã‡ERÄ°K BAZLI TEKRAR KONTROLÃœ (DUPLICATE CHECK) ---
                new_links_count = 0
                temp_links_to_add = []
                
                for link in links:
                    if link not in seen_links:
                        seen_links.add(link)
                        # Linki ekle
                        temp_links_to_add.append({
                            'Link': link,
                            'Sayfa': page_num,
                            'SÄ±ralama': (page_num - 1) * 24 + len(temp_links_to_add) + 1,
                            'Tarama Tarihi': datetime.now().strftime("%Y-%m-%d %H:%M")
                        })
                        new_links_count += 1
                
                # EÄŸer sayfa dolu geldi ama hepsi zaten bizde varsa -> TUZAK SAYFA
                if new_links_count == 0 and len(links) > 0:
                    console.print(f"[bold red]â›” TUZAK TESPÄ°TÄ°! Bu sayfadaki {len(links)} linkin hepsi zaten var. Trendyol bizi eski sayfaya attÄ±.[/bold red]")
                    console.print(f"[bold yellow]ğŸ”„ Tuzak nedeniyle Oturum sÄ±fÄ±rlanÄ±p tekrar deneniyor...[/bold yellow]")
                    
                    # --- AYNI OTURUM YENÄ°LEME MANTIÄI ---
                    await context.close()
                    await asyncio.sleep(2)
                    context = await browser.new_context(**context_args)
                    await apply_stealth(context)
                    page = await context.new_page()
                    scraper = SearchScraper(page)
                    
                    console.print(f"[cyan]ğŸ”„ Yeni oturumla {page_num}. sayfa tekrar deneniyor...[/cyan]")
                    # Tekrar dene
                    links = await scraper.get_product_links(keyword, page_num)
                    
                    # Tekrar kontrole gerek yok, bu turu pas geÃ§ip bir sonraki sayfaya veya aynÄ± sayfaya bakacaÄŸÄ±z
                    # Ama burada links'i tekrar iÅŸleyebiliriz. Basitlik iÃ§in:
                    # EÄŸer yeni oturumda link geldiyse, onlarÄ± tekrar sÃ¼zgeÃ§ten geÃ§ir.
                    if links:
                        # Tekrar sÃ¼z
                        for link in links:
                            if link not in seen_links:
                                seen_links.add(link)
                                temp_links_to_add.append({
                                    'Link': link,
                                    'Sayfa': page_num,
                                    'SÄ±ralama': (page_num - 1) * 24 + len(temp_links_to_add) + 1,
                                    'Tarama Tarihi': datetime.now().strftime("%Y-%m-%d %H:%M")
                                })

                # Listeye ekle
                all_links.extend(temp_links_to_add)
                
                if len(temp_links_to_add) > 0:
                    console.print(f"[green]âœ… Sayfa {page_num} tamamlandÄ±. {len(temp_links_to_add)} YENÄ° link eklendi. (Toplam: {len(all_links)})[/green]")
                else:
                    console.print(f"[yellow]âš ï¸ Sayfa {page_num} tamamlandÄ± ama YENÄ° link Ã§Ä±kmadÄ±.[/yellow]")
                
                # Excel'e anlÄ±k kaydet
                df = pd.DataFrame(all_links)
                df.to_excel(output_file, index=False)
                
                # --- DB KAYIT (LangChain_backend Entegrasyonu) ---
                try:
                    db = SessionLocal()
                    added_count = 0
                    
                    for i, item in enumerate(temp_links_to_add):
                        l = item['Link']
                        
                        # --- 1. SIRA HESAPLAMA (Rank Calculation) ---
                        # Sayfa baÅŸÄ± 24 Ã¼rÃ¼n varsayÄ±mÄ±yla genel sÄ±ralama
                        # FormÃ¼l: (Sayfa - 1) * 24 + (SÄ±fÄ±rdan BaÅŸlayan Ä°ndeks) + 1
                        current_rank = (page_num - 1) * 24 + i + 1
                        
                        # --- 2. ÃœRÃœN KONTROLÃœ / EKLEME ---
                        product = db.query(Product).filter(Product.url == l).first()
                        
                        if not product:
                            # Extract ID from URL if possible, or use random
                            import re
                            t_id_match = re.search(r'-p-(\d+)', l)
                            t_id = t_id_match.group(1) if t_id_match else f"auto_{datetime.now().timestamp()}"
                            
                            product = Product(
                                product_code=t_id,
                                url=l,
                                first_seen_at=datetime.utcnow()
                            )
                            db.add(product)
                            db.commit() # ID almak iÃ§in commit lazÄ±m
                            db.refresh(product)
                            added_count += 1
                        
                        # --- 3. METRÄ°K KAYDI (Sadece SÄ±ralama Ä°Ã§in) ---
                        # EÄŸer bugÃ¼n iÃ§in bu Ã¼rÃ¼nÃ¼n kaydÄ± varsa gÃ¼ncelle, yoksa yeni aÃ§
                        # AmaÃ§: Rank bilgisini kaybetmemek
                        
                        # Not: Scraper (scrape_ultra) daha sonra gelip bu satÄ±rÄ± detaylarla dolduracak
                        # Biz ÅŸimdilik sadece "Yer Tutucu" ve "SÄ±ralama Bilgisi" ekliyoruz.
                        
                        today_metric = DailyMetric(
                            product_id=product.id,
                            sales_rank=current_rank, # Ä°ÅŸte sihirli dokunuÅŸ burasÄ±! ğŸ¯
                            recorded_at=datetime.utcnow()
                        )
                        db.add(today_metric)
                    
                    if added_count > 0:
                        db.commit()
                        console.print(f"[green]ğŸ’¾ VeritabanÄ±na {added_count} yeni Ã¼rÃ¼n eklendi. (SÄ±ralama Bilgisiyle)[/green]")
                    else:
                        db.commit() # Mevcut Ã¼rÃ¼nlerin rank bilgisini kaydetmek iÃ§in
                        console.print(f"[dim]ğŸ’¾ {len(temp_links_to_add)} Ã¼rÃ¼nÃ¼n sÄ±ralama bilgisi gÃ¼ncellendi.[/dim]")
                        
                    db.close()
                except Exception as e:
                    console.print(f"[red]âŒ DB KayÄ±t HatasÄ±: {e}[/red]")
                # -----------------------------------------------
                
                # --- PERÄ°YODÄ°K BAKIM (Her 20 Sayfada Bir) ---
                if page_num % 20 == 0 and page_num < max_pages:
                    console.print(f"[bold magenta]ğŸ› Periyodik Temizlik: {page_num} sayfa bitti. Oturum yenileniyor...[/bold magenta]")
                    await context.close()
                    await asyncio.sleep(3)
                    
                    # Yeni Oturum
                    context = await browser.new_context(**context_args)
                    await apply_stealth(context)
                    page = await context.new_page()
                    scraper = SearchScraper(page)
                    console.print("[magenta]âœ¨ Oturum tazeledi. Devam ediliyor...[/magenta]")

                # Ä°nsan Taklidi (Bekleme) - %40 AZALTILDI
                wait_time = random.uniform(1.2, 2.4)  # âœ… Optimizasyon: 2-4sn -> 1.2-2.4sn
                await asyncio.sleep(wait_time)
                
        except KeyboardInterrupt:
            console.print("[bold yellow]ğŸ‘‹ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu.[/bold yellow]")
        finally:
            # Ã‡erezleri gÃ¼ncelle
            await context.storage_state(path=STATE_FILE)
            await browser.close()
            
    console.print(f"\n[bold blue]ğŸ‰ Toplam {len(all_links)} link toplandÄ±![/bold blue]")
    console.print(f"ğŸ“‚ Dosya: {output_file}")

if __name__ == "__main__":
    # ğŸ§ª TEST SENARYOSU: Stratejik 3 Kategori
    target_categories = ["elbise", "tayt", "kazak"]
    
    console.print(f"[bold yellow]ğŸš€ SÄ°STEM TESTÄ° BAÅLIYOR: {len(target_categories)} Kategori, Kategori BaÅŸÄ±na 200 Sayfa[/bold yellow]")
    
    for cat in target_categories:
        console.print(f"\n[bold blue]ğŸ¯ Kategori Ä°ÅŸleniyor: {cat.upper()}[/bold blue]")
        try:
            asyncio.run(harvest_links(cat, max_pages=200))
        except Exception as e:
            console.print(f"[red]âŒ {cat} kategorisinde kritik hata: {e}[/red]")
            
    console.print("\n[bold green]ğŸ MÄ°SYON TAMAMLANDI: TÃ¼m kategoriler tarandÄ±![/bold green]")
